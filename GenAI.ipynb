{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ABZ848B_p76c",
    "outputId": "5d46bb5f-fcc0-4894-e06a-f8dd92efe085"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
      "Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed bitsandbytes-0.45.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.4.0-py3-none-any.whl (487 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
      "Successfully installed datasets-3.4.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n",
      "Collecting evaluation\n",
      "  Downloading evaluation-0.0.2.tar.gz (2.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from evaluation) (1.26.4)\n",
      "Collecting glog (from evaluation)\n",
      "  Downloading glog-0.3.1-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting python-gflags>=3.1 (from glog->evaluation)\n",
      "  Downloading python-gflags-3.1.2.tar.gz (52 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from glog->evaluation) (1.17.0)\n",
      "Downloading glog-0.3.1-py2.py3-none-any.whl (7.8 kB)\n",
      "Building wheels for collected packages: evaluation, python-gflags\n",
      "  Building wheel for evaluation (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for evaluation: filename=evaluation-0.0.2-py3-none-any.whl size=2444 sha256=f5e80b32a02aac6c587140a5a70639c15920a04795f8b146178ec1a8ff3f02a4\n",
      "  Stored in directory: /root/.cache/pip/wheels/2b/0e/cf/ca6dd6807f277503b984a8e158cd5f8c12907df59326de5fba\n",
      "  Building wheel for python-gflags (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for python-gflags: filename=python_gflags-3.1.2-py3-none-any.whl size=57370 sha256=4f40e1c98e9b85f796085aad7af0a975e29e86d29cc14d54ee30d054bb0a47d8\n",
      "  Stored in directory: /root/.cache/pip/wheels/08/32/87/be6254803d81af495c135b8b662d4a076e7a91dc7766f05a25\n",
      "Successfully built evaluation python-gflags\n",
      "Installing collected packages: python-gflags, glog, evaluation\n",
      "Successfully installed evaluation-0.0.2 glog-0.3.1 python-gflags-3.1.2\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.28.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.13)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.71.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (4.25.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\n",
      "Collecting colorama (from sacrebleu)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: portalocker, colorama, sacrebleu\n",
      "Successfully installed colorama-0.4.6 portalocker-3.1.1 sacrebleu-2.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes\n",
    "!pip install datasets\n",
    "!pip install evaluation\n",
    "!pip install evaluate\n",
    "!pip install tensorboard\n",
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0BH728g687oP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import openai\n",
    "import random\n",
    "import zipfile\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import Dataset\n",
    "from accelerate import Accelerator\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2TokenizerFast,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "collapsed": true,
    "id": "CkaWxij1ffCN",
    "outputId": "ba3b7805-7a23-4b8a-e9d6-527073a179f5"
   },
   "outputs": [
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-18904db3749c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mzip_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/txt.zip\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/de-fr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1311\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1313\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1314\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m                 \u001b[0;31m# set the modified flag so central directory gets written\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1378\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Extract and Load Dataset\n",
    "\n",
    "zip_path = \"/content/txt.zip\"\n",
    "with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"/content/de-fr\")\n",
    "\n",
    "extracted_files = os.listdir(\"/content/de-fr\")\n",
    "print(\"Extracted Files:\", extracted_files)\n",
    "\n",
    "de_folder = \"/content/de-fr/txt/de\"\n",
    "fr_folder = \"/content/de-fr/txt/fr\"\n",
    "\n",
    "def load_and_clean_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Loads all .txt files from a given folder, removes XML-like tags,\n",
    "    and returns a dictionary mapping filenames to cleaned text.\n",
    "    \"\"\"\n",
    "    texts = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "                cleaned_content = re.sub(r\"<[^>]+>\", \"\", content)\n",
    "                cleaned_content = \" \".join(cleaned_content.split())\n",
    "                texts[filename] = cleaned_content\n",
    "    return texts\n",
    "\n",
    "# Load and clean files from both folders\n",
    "german_texts = load_and_clean_folder(de_folder)\n",
    "french_texts = load_and_clean_folder(fr_folder)\n",
    "\n",
    "# Pair files by filename\n",
    "paired_data = []\n",
    "common_files = set(german_texts.keys()).intersection(set(french_texts.keys()))\n",
    "for filename in common_files:\n",
    "    paired_data.append({\n",
    "        \"german\": german_texts[filename],\n",
    "        \"french\": french_texts[filename]\n",
    "    })\n",
    "\n",
    "print(f\"Total paired files found: {len(paired_data)}\")\n",
    "\n",
    "# Sample 1,000 pairs\n",
    "if len(paired_data) > 1000:\n",
    "    paired_data = random.sample(paired_data, 1000)\n",
    "\n",
    "df = pd.DataFrame(paired_data)\n",
    "df = df.applymap(str.strip)\n",
    "print(\"Sample Data:\\n\", df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155,
     "referenced_widgets": [
      "72c17a3b6d324182a48d9821977042b4",
      "423e9a8b6c324f89bfc6fedb36a98369",
      "1542e7b0ca884ef59420a97677339480",
      "fdadb4af0e3e4703abb3e0b5868e74ff",
      "bd2dbd01424d4a40a1dbb16940b65bef",
      "6342fb63a2d44717a6418dd1db5f1731",
      "5a18ed63dfe148c9a70b2b943dcd580d",
      "06c4a21f641243b3aafec776475ac6f4",
      "1cbfab9b8816495aa8a497fa1e0e401f",
      "a49b1de31b9042fc98385e3c85eba0cc",
      "c80bec2a91744069a94bfe853c36a14f",
      "18aa355e55c44e579865a03ce4294db9",
      "8dcd6ab907d448beaa2ba71a3a510d2a",
      "4631cbb27a79445fbf29c8a5206f0db3",
      "e3d90d8e8cb344c6a7611cee20c101d9",
      "4798ba2227f644e2999ff1d4494794a3",
      "76c4a91716014544b61e2de4ce6671d8",
      "5e1ecc065b5e4f2a9a9c0cd100aae139",
      "2f01da6db47647f2bcafa5a19685392c",
      "d0086ec83b0d4286a8821a649861068b",
      "1410f2e663b14711aee2d19a803274c3",
      "5c2174b7d39a4276a2638cec5421a0db"
     ]
    },
    "id": "W2mOMvrIgfN8",
    "outputId": "8abc19fa-81b2-448f-ec66-a67606df15a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 800\n",
      "Test samples: 200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c17a3b6d324182a48d9821977042b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18aa355e55c44e579865a03ce4294db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample example from training set: {'german': 'Tagesordnung der nächsten Sitzung: siehe Protokoll', 'french': 'Ordre du jour de la prochaine séance: voir procès-verbal', 'prompt': 'translate German to French: Tagesordnung der nächsten Sitzung: siehe Protokoll', 'target': 'Ordre du jour de la prochaine séance: voir procès-verbal'}\n"
     ]
    }
   ],
   "source": [
    "pairs = df.to_dict(orient=\"records\")\n",
    "dataset = Dataset.from_list(pairs)\n",
    "\n",
    "# Split the dataset\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=seed)\n",
    "print(\"Train samples:\", len(split_dataset[\"train\"]))\n",
    "print(\"Test samples:\", len(split_dataset[\"test\"]))\n",
    "\n",
    "def add_prompt(example):\n",
    "    example[\"prompt\"] = \"translate German to French: \" + example[\"german\"]\n",
    "    example[\"target\"] = example[\"french\"]\n",
    "    return example\n",
    "\n",
    "split_dataset = split_dataset.map(add_prompt)\n",
    "print(\"Sample example from training set:\", split_dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560,
     "referenced_widgets": [
      "7c030203d6ce42da9f4dd17eb86bea14",
      "8d4d3679c9c14f908a4809520a7ef3c0",
      "2b31fcd1062c4d078f1eb7ff79f2ae85",
      "310d3b466da24146ab03b3b28766ce78",
      "35d3c9162e704e5e99daab255988da01",
      "7ca9726e9a8843dba468f9b6690050e3",
      "b5a495dc54af4db6a6dac9cf9ed0e0ab",
      "7cb621181dfb4a7c910ff1bdc954e138",
      "871f5d8a2e6a415da1a446d0368672e9",
      "8d52415906634409a4ba17b256be4dd0",
      "4cf5028328a04acead69c5b3891c03c7",
      "433e61a5f3464edfb4a04c017d74edb0",
      "f19bc7cf1b5b48349a8cb4a533b0d952",
      "1ff2f6ce4c94476e858ea5befb5a11c1",
      "322db46bfba440b1b0366d345ac0f992",
      "e2544f362da94e15a71a14237e56f7b2",
      "583c825b7edb411fa9a751c651525a61",
      "6258ac3817f94b7693b920fde34ce70e",
      "487373b2b1df4e70b3352e887e2b33fa",
      "963c86a29d004403b1386b797aaa7660",
      "eb49ee9033f544cb802576c8bf83f01c",
      "6d62fd6002544a7fb703e9ef4c3e6336",
      "9518ab4afb654f138d864114a0ebf36f",
      "ea17782071644ce0a15c40e6d245c507",
      "7071f7c707774d03bd26ba57a0553f1a",
      "c534026c2c3d416cbe2bfbf1aeb3344a",
      "e1cf87b369484277a187b092a0834774",
      "544f48a3bff44271b676b0890176e7f7",
      "4178a4dae75c4cf0813d7045263e65f4",
      "b018ca40877e4dcc882bc47a150a6478",
      "5f5df91848b147ad901e9bfda12dd1cc",
      "c15f3339307f407d8e4d065d917c86d7",
      "c300a7210a5148a49b45485d59e1663b",
      "6650c9c0dd434c4aac41cd2b889687e9",
      "2bc18540c5fa429e99a14077a22a9d0e",
      "7c9aeb656b72408c9eb3a797d8496c8d",
      "ec3a854a46e048ac9960f1c24100ae47",
      "6ec50954247846a69c925674f8b4b975",
      "5519b45097994ef083615d0bc04af99c",
      "dad7236164de463f835e2f4ea5f04c00",
      "cb5233cfefca4c6db39acafba64c458c",
      "2960425f75cf45ca89babd09703f8f71",
      "5e84e4bae6f6456b90887fe780c642ef",
      "141b103f81bb4a84a8ae4152714851a4",
      "6d70a6db306c499ab864e0b0ae7fe08e",
      "9e931c138b1d479bbfb1d4ebca0484b4",
      "07faba6fc9f7492b842e29f107a2171c",
      "062f9e61954748bfaf41f78bc959fd91",
      "26b4897b1d2a4f5381e441804d4506b6",
      "b103bedc04ba4b26be03f9806ac736cb",
      "49529319863b4666995e1b8e3f239cff",
      "d693d870f61b40a2a8cfffeb757b3c1f",
      "7278d1eb506047d2a599a1f8f0d6e3d7",
      "5402ba9cf2da4d2a90624fe93d23f2ae",
      "e397f969d88a4c1e8eeb1e7fa2fc5c7f",
      "f2fa3253832b43cf926846c59f390645",
      "3f523800587d44379dbc4bdeeee0539d",
      "4f68ce129c294a829bf6cae59b2e7124",
      "5d2355d4f61149049f89a9862971c13a",
      "070d41f5661f4273b888aaebd9809416",
      "d7a777aa92fe40d99d8aac4a0fe227af",
      "9d15831ee319446cb4c91256fee84e9c",
      "b4496215551543d5914d830a9e822d6e",
      "a847dd81a654482aa40236c690873e7a",
      "d1493a58001c406d968cba5bcc8c81d5",
      "d58a6cafea094a6291cea8c1102b7bef",
      "53da697fd2a74967a2b3e5cb7c0f1919",
      "33ee407dd86f491a8bcc21ebc090dd78",
      "a836c2c719de4595838e9a4cdffbdd44",
      "ccc1032409094f6cb4389e3b339551e8",
      "878e1c7613b64b839bd1e8995843c606",
      "95bea193c9ea452fa33fea9912ff8fa9",
      "d876483f5c87435a80464ba0babc47e6",
      "f918f4d3215c4eefb8aa70cfa7740092",
      "cb624259d86c400db6c403798d7c1804",
      "10168094085f4a7ab6d17e7375a8853c",
      "f891a3d3097146afb0fcb1e3cddedbad",
      "45e8e3d4c922407590ee5ab221a5cd41",
      "d1f48824d291499d8445ca3860cb4663",
      "ee50ab45fa6449afb708b026b5fd1429",
      "d8e95b84582f47cf941c9a1c2a520d3d",
      "a5ba2211b25f491b9dcc0b8023725e8b",
      "c0738e9b3833495ab89bbcd19502116b",
      "401b9bc90cfa4c4198d6bd497d690435",
      "89d93b278e8e4a9e9d01a67292534b39",
      "903c33ea44784abc968d00fd7a341eff",
      "a32aec3c5a1246ef88672385bd471720",
      "6cbae4ad910f4e0582e55530e7834984",
      "a238638cc53446538a6441a507d3e75c",
      "f4865a485d41450e830d7249c55002ba",
      "f10d111998484d2fb2ecfcf14dd5b0be",
      "81a6cab6b9af4b5da219eb233cb3491d",
      "8dc7bfb4fcb24987827a2e3ea3ae3892",
      "d49f6a9e2e0446e189ac542ef722a373",
      "fe1b97ca518d48ec9e2b7496c8eeaa45",
      "f7a5584d67924ee78ce1465f0adcde44",
      "d0bc879a9cee456588b1e0d5942cb3d2",
      "a10bff3c1dc442e1ac3aa12b0737db72",
      "a0b20656c0764a7f90b44dce948621cc",
      "b8a193068a734fa58ea25859c756f75b",
      "9dad61bfb4fd4cbf9c444ad5f188e11b",
      "5bef2ca867df414eba4c4bcc3cca1dd0",
      "c10292353d3c4a96b42827d2b652249d",
      "966254ac4e194db3ad49729abd03cc18",
      "2a86a0860c774aecb69ca0b8d3a25d59",
      "4960c8e93b754eb1a064f19607f98e74",
      "b727b711e2354517b3052edf7b3a0b10",
      "c67ba9ebae2f46b3bc39d2649c1fb6af",
      "331877ff2a13481d941b0737f3502786",
      "330ce8029d2646ee980445ab00e5346a",
      "17ff20d91553437e86fe56633b03d538",
      "29e95e0610f142b9bc49f5c417bb06fb",
      "0987195ad54a4209a8ffab478f413863",
      "fb147d39c2f441feb3cdbbb6e044e278",
      "4e5370174f404915816d915f278789b3",
      "d53cc1d9ec9f4d45af21e01461f6ace0",
      "5775d0c46974457aaf84c745e18f2cc4",
      "a69ccaeb211b4ef6a286378b469c8732",
      "6b53f59704ca4615b2081ccf6a60ca18",
      "d66014abea7d4001a71ebdb842131adb",
      "c89989cef73f47d79f17a8cd541c60ab",
      "3b6d1c6705034a709c2a7df13532aae8",
      "e586ca2ee21843b69bdb2234e251b374",
      "d88b090a80b24c3582cbad1fe412afa7",
      "9144d90dad9b4cb2a8c07a0a5a6bc1ec",
      "544fe26caca44424b113bb175dcc173c",
      "cae3d6d582e446d4ab1cb6805a6ef25c",
      "642a9a0604b843139b447d4012c0c704",
      "523eb77e5dc44cd49bdd5b2071445c2b",
      "d32ea816cbae434ea27e1a6b1ff9fdf1",
      "aee9c49bcbc24181af5e3237bf3d8d12",
      "c7e01243bf4c49c997d0c6770809c437",
      "98609c0879504c95ac8a8a43d1de31eb",
      "0cabe6251f0a4b2885ae1ac83749d3a8",
      "f6686d5d5c9c42d68535351095777929",
      "c92302a20e4245328f4ec034a6017bc6",
      "07e3d390d3c5420eb8f17d4d9bc4cedb",
      "72019ffc1f32453c82ed7e935183b99e",
      "e0ea29c2fe6e4c3e928489e345008272",
      "dc8e9dbe2d1c406e82036e5ee0af7be9",
      "53f8a496e2ad49d88742982dcd11328b",
      "d45037371afd4d0ba3dcba46c8d4cb78",
      "8cc098c5866a42ba9ae64e2ac6533b4f"
     ]
    },
    "id": "CWIw-h4f9PrC",
    "outputId": "56f86c5d-9e9e-4114-aa01-7d86e65ec18b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c030203d6ce42da9f4dd17eb86bea14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433e61a5f3464edfb4a04c017d74edb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9518ab4afb654f138d864114a0ebf36f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6650c9c0dd434c4aac41cd2b889687e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d70a6db306c499ab864e0b0ae7fe08e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2fa3253832b43cf926846c59f390645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53da697fd2a74967a2b3e5cb7c0f1919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e8e3d4c922407590ee5ab221a5cd41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a238638cc53446538a6441a507d3e75c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a193068a734fa58ea25859c756f75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ff20d91553437e86fe56633b03d538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6d1c6705034a709c2a7df13532aae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98609c0879504c95ac8a8a43d1de31eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model and Tokenizer for microsoft/phi-2\n",
    "model_name = \"microsoft/phi-2\"\n",
    "\n",
    "# Quantization configuration\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True,\n",
    ")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quant_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Freeze base model parameters except for LoRA layers\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" not in name:\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "113c250bb0c24da5ac0ee578127d7c69",
      "d6185991a9564bdbbbebfc065b6465a8",
      "7bdc6718c487447baa0310310f0b711b",
      "efc189dcd96340929727ae5709c4df6c",
      "eed018dc429f4e2e8294ed54c76b1cb0",
      "ff1c277168d2407986319897a70bc32f",
      "58ad96fc44ac4b3d8892e3e420b6f60a",
      "0760aa3294184b90bedc63245ca74987",
      "fa0b1cc66cca4b90bf31613ba7d9ae09",
      "a67b7ac6ab5244ac9f3180059b4a122c",
      "9049c3d3543d400eafdc17aec5a735ac"
     ]
    },
    "id": "nOJrURDJhCbC",
    "outputId": "f8ab94f5-72dc-4d37-cd30-cfd84f3dbe13"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113c250bb0c24da5ac0ee578127d7c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the Dataset using batched processing\n",
    "def tokenize_function(examples):\n",
    "    prompts = [prompt + \" \" + tokenizer.eos_token + \" \" + target for prompt, target in zip(examples[\"prompt\"], examples[\"target\"])]\n",
    "\n",
    "    return tokenizer(prompts, truncation=True, max_length=1024, padding=True)\n",
    "\n",
    "# Tokenize the train dataset with batched=True\n",
    "tokenized_train = split_dataset[\"train\"].map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520,
     "referenced_widgets": [
      "a172b00e8fa441b898f55dcde410780e",
      "0bd2dfd616604c02b6e4e95208b56194",
      "f32a55c7b03c4c6cb722e7a7ad7547e5",
      "2132b37c87904ce0b3d3a91e77a287de",
      "e3e213fb96b14951a7bed0ddd6ef52ba",
      "b90060bb067f4a2abf547c064abfe86b",
      "69723a41df2c4bd898ee41414024c9f0",
      "7105eba543dc46718f6056c14ec1c5d0",
      "4f33bd2d2b4a48479fb75480e4e80d59",
      "5074898c9cb6493fa3b167eed0584ae0",
      "e3722373bfb34ad582fcf3f7461a6e28",
      "cd908c8616644ebda6bf5c941e57e325",
      "19f4e8f366524f59be0b594705436eb2",
      "4a1a6cfce5054d0cab3f82b686177182",
      "fd48122eafa94dc087b6054f76e38e1c",
      "88339dc527ed451d81b036c05652e414",
      "fb53a7e281e5434eac1fa4048b69ab41",
      "67fca7fbcd00408fa64a9f5b1b977e71",
      "253228ad7ea74a85b29dc76718880370",
      "79071ce4961741d5982981c49515d245",
      "034c2e2af3284522af49aaccb9888eb8",
      "479434fee7ad4e93bafbe46070a37a47"
     ]
    },
    "id": "_O0_c3Lh-RcH",
    "outputId": "bfe86841-64e4-40f2-9b48-f49bf79a1043"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/accelerate/utils/modeling.py:1536: UserWarning: Current model requires 64 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a172b00e8fa441b898f55dcde410780e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd908c8616644ebda6bf5c941e57e325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Base Model:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-abd43f3d15d8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mbatch_refs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mbatch_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_translation_base\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_prompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_refs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-abd43f3d15d8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mbatch_refs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mbatch_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_translation_base\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_prompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_refs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-abd43f3d15d8>\u001b[0m in \u001b[0;36mgenerate_translation_base\u001b[0;34m(german_sentence, num_beams, max_length)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         output_ids = model.generate(\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2285\u001b[0m             \u001b[0;31m# 13. run beam sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2286\u001b[0;31m             result = self._beam_search(\n\u001b[0m\u001b[1;32m   2287\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2288\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3505\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Unchanged original behavior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3506\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3508\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/phi/modeling_phi.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m    809\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/phi/modeling_phi.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m                 )\n\u001b[1;32m    565\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    567\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/phi/modeling_phi.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         attn_outputs, self_attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    250\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/phi/modeling_phi.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mquery_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Evaluating base model before fine-tune\n",
    "import sacrebleu\n",
    "# Define the model name\n",
    "model_name = \"microsoft/phi-2\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the tokenizer for the base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "model.eval()\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Function to generate translations\n",
    "def generate_translation_base(german_sentence, num_beams=4, max_length=150):\n",
    "    prompt_text = (\n",
    "        \"Translate the following German sentence into French accurately and uniquely.\\n\\n\"\n",
    "        f\"German: {german_sentence}\\n\\nFrench:\"\n",
    "    )\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    try:\n",
    "        output_ids = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            repetition_penalty=1.2,\n",
    "            no_repeat_ngram_size=2,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        if generated_text.startswith(prompt_text):\n",
    "            generated_text = generated_text[len(prompt_text):]\n",
    "        return generated_text.strip().split(\"\\n\")[0].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating translation: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the test dataset (assuming split_dataset is defined)\n",
    "test_dataset = split_dataset[\"test\"]\n",
    "\n",
    "# Store predictions and references\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "# Process in batches\n",
    "batch_size = 8\n",
    "for i in tqdm(range(0, len(test_dataset), batch_size), desc=\"Evaluating Base Model\"):\n",
    "    batch_prompts = [test_dataset[i+j][\"german\"] for j in range(batch_size) if i+j < len(test_dataset)]\n",
    "    batch_refs = [test_dataset[i+j][\"target\"] for j in range(batch_size) if i+j < len(test_dataset)]\n",
    "\n",
    "    batch_predictions = [generate_translation_base(prompt, num_beams=4, max_length=150) for prompt in batch_prompts]\n",
    "\n",
    "    for pred, ref in zip(batch_predictions, batch_refs):\n",
    "        if pred is not None and pred != \"\":\n",
    "            predictions.append(pred)\n",
    "            references.append(ref)\n",
    "\n",
    "# Compute BLEU score\n",
    "if predictions:\n",
    "    bleu = sacrebleu.corpus_bleu(predictions, [[ref] for ref in references])\n",
    "    print(\"BLEU Score for Base Model:\", bleu.score)\n",
    "else:\n",
    "    print(\"No valid translations generated. Skipping BLEU score.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 769,
     "referenced_widgets": [
      "79d3bc2a4bba425daed7f2217736b73f",
      "307eaa27297c4339bb33709210edd21a",
      "79a822b3a3d741168ed4655b46630f06",
      "1d36a43d181f4590a6aded012096c7cf",
      "61e45e7843be4eb6b26573614cdf2aa6",
      "611837279376428abd6deb93fbf3c3a7",
      "095c77227736442f8a3ef19d6f7933e9",
      "60daa8b365414646bee5133304565971",
      "6c0202873e8b49d8ae6426bb97ef930b",
      "47d85739af234a7b8eed571781b162e5",
      "9b9acbf1ae204e7da2c046f3c8028c34"
     ]
    },
    "id": "aY7Bq47Pibid",
    "outputId": "a6f5fa9c-7f44-4aee-8c94-89dba5bd8fe4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d3bc2a4bba425daed7f2217736b73f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 9175040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning on Dataset A to create Model B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maida-farshiali\u001b[0m (\u001b[33maida-farshiali-rptu\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250314_171805-updlyfvw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aida-farshiali-rptu/huggingface/runs/updlyfvw' target=\"_blank\">./phi-2-finetuned-b</a></strong> to <a href='https://wandb.ai/aida-farshiali-rptu/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aida-farshiali-rptu/huggingface' target=\"_blank\">https://wandb.ai/aida-farshiali-rptu/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aida-farshiali-rptu/huggingface/runs/updlyfvw' target=\"_blank\">https://wandb.ai/aida-farshiali-rptu/huggingface/runs/updlyfvw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='800' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [800/800 1:14:14, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.802500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.327900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.144700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.043600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.977800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.988400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.865400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=800, training_loss=2.147381553649902, metrics={'train_runtime': 4482.0323, 'train_samples_per_second': 0.357, 'train_steps_per_second': 0.178, 'total_flos': 2.6127108145152e+16, 'train_loss': 2.147381553649902, 'epoch': 2.0})\n",
      "Fine-tuning complete. Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune base model\n",
    "\n",
    "from transformers import (\n",
    "    TrainerCallback\n",
    ")\n",
    "\n",
    "# Clear GPU Cache After Each Step\n",
    "class ClearCacheCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Model and Tokenizer Setup\n",
    "model_name = \"microsoft/phi-2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Quantization Configuration\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "# Load the model with the quantization configuration.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0}  # Forces all parameters onto GPU 0\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "# LoRA (PEFT) Configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA configuration to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Freeze Non-LoRA Parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Check LoRA Layers for Gradient Tracking\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "if trainable_params == 0:\n",
    "    raise ValueError(\"No parameters are trainable. Check your LoRA configuration.\")\n",
    "\n",
    "# Data Collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Fine-Tuning Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./phi-2-finetuned-b\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-5,\n",
    "    fp16=True,\n",
    "    save_steps=1000,\n",
    "    logging_dir=\"./logs/model_B_logs\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[ClearCacheCallback]\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"Starting fine-tuning on Dataset A to create Model B...\")\n",
    "train_results = trainer.train()\n",
    "print(train_results)\n",
    "\n",
    "trainer.save_model(\"./my_finetuned_phi_2\")\n",
    "print(\"Fine-tuning complete. Model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q9pZdPCh-rwf",
    "outputId": "75db2036-6781-4472-ba44-c566dadd9c39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hu6YpsDZ-t4q",
    "outputId": "451f98fc-33e8-4ea0-b08f-49b10d19df89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to /content/drive/MyDrive/my_finetuned_phi_2\n"
     ]
    }
   ],
   "source": [
    "save_path = \"/content/drive/MyDrive/my_finetuned_phi_2\"\n",
    "\n",
    "local_model_path = \"./my_finetuned_phi_2\"\n",
    "\n",
    "tokenizer.save_pretrained(local_model_path)\n",
    "\n",
    "shutil.copytree(local_model_path, save_path)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351,
     "referenced_widgets": [
      "e535151698ca4cc5b174e56e03cdd925",
      "b2cab5b51df2460ca86bc11dbe78460f",
      "65d52a346d574a77b3ee794e730e65a4",
      "8f495cb96c894fe6980493ec58ae4f32",
      "15ec781fde4046e5a9f72a479966843b",
      "f4068caffc2d471d9cd7309771916a64",
      "5f2ed9cf278a4843af0f4df578636c75",
      "c9877a81e3b34564aa6f3b386ca9d200",
      "44d712f5614d47dc997cd5b439bce7b4",
      "1ae5751ffae34906a9dc0dc4f7c79f78",
      "3aea289ba1844e4f9d618ebb37732c20",
      "582f0295546a46dca2c01d606c7fbb18",
      "3c6076ac1bfc48968bc59e6f6d68f089",
      "9fe037450a3241169dba6db1b4429000",
      "2ae90e556ab74a26944878b7662cc437",
      "11be7624eac944148c75ac53c3c7fb5f",
      "b9d68a4dc3f9440ebca68c865a7bf61c",
      "6de8eb897314408fa829503ef36ce3f4",
      "07d7b4d9fc2d47fcb94e30b9f32c5057",
      "b0f9a93716a54815a89bea1977ce838b",
      "36948d39180046598d20da7c7909abe7",
      "5cf196b3b4b94115a915ea2e7d44e5a4"
     ]
    },
    "id": "dRxPlLkJ8fqU",
    "outputId": "dc2b7e2c-bcf2-448b-8950-395f159ec4fa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e535151698ca4cc5b174e56e03cdd925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "582f0295546a46dca2c01d606c7fbb18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Model B:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=50) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=50) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=50) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=50) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=50) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=50) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=50) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=50) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=50) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=50) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=50) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=50) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=50) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SacreBLEU Score for Model B: 7.568234239548304e-196\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on fine-tuned data\n",
    "\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "model_path = \"/content/drive/MyDrive/my_finetuned_phi_2\"\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "def generate_translations_batch(prompts, num_beams=4, max_length=128, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    Generates translations for a batch of input prompts.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        output_ids = model.generate(\n",
    "            inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            repetition_penalty=1.2,\n",
    "            no_repeat_ngram_size=2,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            max_new_tokens=max_new_tokens\n",
    "        )\n",
    "        outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    return outputs\n",
    "\n",
    "test_dataset = split_dataset[\"test\"]\n",
    "\n",
    "all_prompts = [f\"Translate the following German sentence into French:\\n\\nGerman: {ex['german']}\\n\\nFrench:\"\n",
    "               for ex in test_dataset]\n",
    "all_references = [ex[\"target\"] for ex in test_dataset]\n",
    "\n",
    "predictions = []\n",
    "batch_size = 16\n",
    "for i in tqdm(range(0, len(all_prompts), batch_size), desc=\"Evaluating Model B\"):\n",
    "    batch_prompts = all_prompts[i: i + batch_size]\n",
    "    batch_outputs = generate_translations_batch(batch_prompts, num_beams=4, max_length=128, max_new_tokens=50)\n",
    "\n",
    "    for prompt, output in zip(batch_prompts, batch_outputs):\n",
    "        output = output.replace(prompt, \"\").strip().split(\"\\n\")[0].strip()\n",
    "        predictions.append(output)\n",
    "\n",
    "if predictions:\n",
    "    bleu_result = bleu_metric.compute(predictions=predictions, references=[[ref] for ref in all_references])\n",
    "    print(\"SacreBLEU Score for Model B:\", bleu_result[\"score\"])\n",
    "else:\n",
    "    print(\"No valid translations generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00we6zVuKPKJ"
   },
   "outputs": [],
   "source": [
    "# Generating synthesize data_1\n",
    "\n",
    "GDRIVE_PATH = \"/content/drive/MyDrive/generated_pairs\"\n",
    "if not os.path.exists(GDRIVE_PATH):\n",
    "    os.makedirs(GDRIVE_PATH)\n",
    "\n",
    "# Set up the OpenAI-compatible client\n",
    "client = openai.Client(\n",
    "    api_key=\"\",\n",
    "    base_url=\"https://api.sambanova.ai/v1\",\n",
    ")\n",
    "\n",
    "existing_sentences = set()\n",
    "\n",
    "def generate_synthetic_data(prompt, max_tokens=4096, temperature=0.9, top_p=0.95, retries=5):\n",
    "    \"\"\"Requests translation pairs from the SamBanova API with retry logic for rate limits.\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"Meta-Llama-3.1-70B-Instruct\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an advanced AI assistant. Your task is to generate diverse, complex, and unique translation pairs between German and French.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except openai.APIError as e:\n",
    "            if \"rate_limit_exceeded\" in str(e):\n",
    "                print(f\"Rate limit exceeded. Retrying... (Attempt {attempt+1}/{retries})\")\n",
    "                time.sleep(30)\n",
    "            else:\n",
    "                print(f\"API request failed due to unexpected error: {e}\")\n",
    "                return None\n",
    "    print(\"Max retries reached. Skipping this batch.\")\n",
    "    return None\n",
    "\n",
    "TOTAL_PAIRS = 1000\n",
    "BATCH_SIZE = 25\n",
    "NUM_CALLS = TOTAL_PAIRS // BATCH_SIZE\n",
    "\n",
    "for i in range(NUM_CALLS):\n",
    "    print(f\"Requesting batch {i+1}/{NUM_CALLS} ({BATCH_SIZE} pairs)...\")\n",
    "    prompt_text = (\n",
    "            \"Generate exactly 25 unique and diverse German–French translation pairs. \"\n",
    "            \"Each pair must be represented as a JSON object with two keys: 'german' and 'french'.\\n\\n\"\n",
    "            \"### INSTRUCTIONS:\\n\"\n",
    "            \"- Ensure that the generated sentences are distinct and vary in structure.\\n\"\n",
    "            \"- Use a range of sentence types such as questions, commands, and statements.\\n\"\n",
    "            \"- Include complex sentence structures, including passive voice, relative clauses, and indirect speech.\\n\"\n",
    "            \"- Incorporate formal and informal vocabulary, ensuring a balance of both.\\n\"\n",
    "            \"- Explore different tenses, moods, and aspects of the verbs (e.g., subjunctive, future tense).\\n\"\n",
    "            \"- The sentences should reflect cultural nuances, including idiomatic expressions or formal phrasing.\\n\"\n",
    "            \"- Do not repeat sentence patterns; every translation must be unique.\\n\\n\"\n",
    "            \"Example translations:\\n\"\n",
    "            \"[\\n\"\n",
    "            \"  {\\\"german\\\": \\\"Hätte ich gewusst, dass er kommt, hätte ich mich besser vorbereitet.\\\",\\n\"\n",
    "            \"   \\\"french\\\": \\\"Si j'avais su qu'il venait, je me serais mieux préparé.\\\"},\\n\"\n",
    "            \"  {\\\"german\\\": \\\"Die Entscheidung, das Angebot anzunehmen, war alles andere als einfach.\\\",\\n\"\n",
    "            \"   \\\"french\\\": \\\"La décision d'accepter l'offre n'a pas été facile du tout.\\\"},\\n\"\n",
    "            \"  {\\\"german\\\": \\\"Wenn wir nicht bald handeln, wird sich die Situation weiter verschlechtern.\\\",\\n\"\n",
    "            \"   \\\"french\\\": \\\"Si nous n'agissons pas bientôt, la situation continuera à se détériorer.\\\"}\\n\"\n",
    "            \"]\\n\\n\"\n",
    "            \"### IMPORTANT RULES:\\n\"\n",
    "            \"- Generate **exactly 25 pairs**. Do not include more or fewer pairs.\\n\"\n",
    "            \"- Ensure **every sentence is unique**. Avoid slight variations of the same structure.\\n\"\n",
    "            \"- The sentences must be **original** and not be derived from common or frequently used phrases.\"\n",
    "        )\n",
    "\n",
    "    generated_pairs = generate_synthetic_data(prompt_text, max_tokens=4096, temperature=0.9, top_p=0.95)\n",
    "\n",
    "    if generated_pairs is None:\n",
    "        print(f\"Skipping batch {i+1} due to API request failure.\")\n",
    "        continue\n",
    "\n",
    "    # Check for duplicates before saving\n",
    "    if generated_pairs in existing_sentences:\n",
    "        print(f\"Duplicate response detected in batch {i+1}. Skipping.\")\n",
    "        continue\n",
    "    existing_sentences.add(generated_pairs)\n",
    "\n",
    "    raw_filename = f\"{GDRIVE_PATH}/generated_pair_{i+1}.txt\"\n",
    "    with open(raw_filename, \"w\", encoding=\"utf-8\") as raw_file:\n",
    "        raw_file.write(generated_pairs)\n",
    "    print(f\"Batch {i+1} saved to {raw_filename}\")\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "print(\"\\nAll batches completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "V_6aAlsw_EFf"
   },
   "outputs": [],
   "source": [
    "# Generating synthesize data_2\n",
    "\n",
    "GDRIVE_PATH = \"/content/drive/MyDrive/generated_pairs_2\"\n",
    "if not os.path.exists(GDRIVE_PATH):\n",
    "    os.makedirs(GDRIVE_PATH)\n",
    "\n",
    "# Set up the OpenAI-compatible client\n",
    "client = openai.Client(\n",
    "    api_key=\"\",\n",
    "    base_url=\"https://api.sambanova.ai/v1\",\n",
    ")\n",
    "\n",
    "existing_sentences = set()\n",
    "\n",
    "def generate_synthetic_data(prompt, max_tokens=4096, temperature=0.9, top_p=0.95, retries=5):\n",
    "    \"\"\"Requests translation pairs from the SamBanova API with retry logic for rate limits.\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"Meta-Llama-3.3-70B-Instruct\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an advanced AI assistant. Your task is to generate diverse, complex, and unique translation pairs between German and French.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except openai.APIError as e:\n",
    "            if \"rate_limit_exceeded\" in str(e):\n",
    "                print(f\"Rate limit exceeded. Retrying... (Attempt {attempt+1}/{retries})\")\n",
    "                time.sleep(30)\n",
    "            else:\n",
    "                print(f\"API request failed due to unexpected error: {e}\")\n",
    "                return None\n",
    "    print(\"Max retries reached. Skipping this batch.\")\n",
    "    return None\n",
    "\n",
    "TOTAL_PAIRS = 1000\n",
    "BATCH_SIZE = 25\n",
    "NUM_CALLS = TOTAL_PAIRS // BATCH_SIZE\n",
    "\n",
    "for i in range(NUM_CALLS):\n",
    "    print(f\"Requesting batch {i+1}/{NUM_CALLS} ({BATCH_SIZE} pairs)...\")\n",
    "    prompt_text = (\n",
    "            \"Generate exactly 25 unique and diverse German–French translation pairs. \"\n",
    "            \"Each pair must be represented as a JSON object with two keys: 'german' and 'french'.\\n\\n\"\n",
    "            \"### INSTRUCTIONS:\\n\"\n",
    "            \"- Ensure that the generated sentences are distinct and vary in structure.\\n\"\n",
    "            \"- Use a range of sentence types such as questions, commands, and statements.\\n\"\n",
    "            \"- Include complex sentence structures, including passive voice, relative clauses, and indirect speech.\\n\"\n",
    "            \"- Incorporate formal and informal vocabulary, ensuring a balance of both.\\n\"\n",
    "            \"- Explore different tenses, moods, and aspects of the verbs (e.g., subjunctive, future tense).\\n\"\n",
    "            \"- The sentences should reflect cultural nuances, including idiomatic expressions or formal phrasing.\\n\"\n",
    "            \"- Do not repeat sentence patterns; every translation must be unique.\\n\\n\"\n",
    "            \"Example translations:\\n\"\n",
    "            \"[\\n\"\n",
    "            \"  {\\\"german\\\": \\\"Hätte ich gewusst, dass er kommt, hätte ich mich besser vorbereitet.\\\",\\n\"\n",
    "            \"   \\\"french\\\": \\\"Si j'avais su qu'il venait, je me serais mieux préparé.\\\"},\\n\"\n",
    "            \"  {\\\"german\\\": \\\"Die Entscheidung, das Angebot anzunehmen, war alles andere als einfach.\\\",\\n\"\n",
    "            \"   \\\"french\\\": \\\"La décision d'accepter l'offre n'a pas été facile du tout.\\\"},\\n\"\n",
    "            \"  {\\\"german\\\": \\\"Wenn wir nicht bald handeln, wird sich die Situation weiter verschlechtern.\\\",\\n\"\n",
    "            \"   \\\"french\\\": \\\"Si nous n'agissons pas bientôt, la situation continuera à se détériorer.\\\"}\\n\"\n",
    "            \"]\\n\\n\"\n",
    "            \"### IMPORTANT RULES:\\n\"\n",
    "            \"- Generate **exactly 25 pairs**. Do not include more or fewer pairs.\\n\"\n",
    "            \"- Ensure **every sentence is unique**. Avoid slight variations of the same structure.\\n\"\n",
    "            \"- The sentences must be **original** and not be derived from common or frequently used phrases.\"\n",
    "        )\n",
    "\n",
    "    generated_pairs = generate_synthetic_data(prompt_text, max_tokens=4096, temperature=0.9, top_p=0.95)\n",
    "\n",
    "    if generated_pairs is None:\n",
    "        print(f\"Skipping batch {i+1} due to API request failure.\")\n",
    "        continue\n",
    "\n",
    "    # Check for duplicates before saving\n",
    "    if generated_pairs in existing_sentences:\n",
    "        print(f\"Duplicate response detected in batch {i+1}. Skipping.\")\n",
    "        continue\n",
    "    existing_sentences.add(generated_pairs)\n",
    "\n",
    "    raw_filename = f\"{GDRIVE_PATH}/generated_pair_2_{i+1}.txt\"\n",
    "    with open(raw_filename, \"w\", encoding=\"utf-8\") as raw_file:\n",
    "        raw_file.write(generated_pairs)\n",
    "    print(f\"Batch {i+1} saved to {raw_filename}\")\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "print(\"\\nAll batches completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z8QjsmZJ4zMr",
    "outputId": "d131dcc6-297b-4480-abdb-11542dd3ac7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset A Train saved at /content/dataset_a_train.json with 800 pairs\n",
      "Dataset A Test saved at /content/dataset_a_test.json with 200 pairs\n",
      "Dataset A Train size: 800 pairs\n",
      "Dataset B size: 1561 pairs\n",
      "Combined dataset size (before deduplication): 2361 pairs\n",
      "Unique combined dataset size: 2248 pairs\n",
      "Dataset C saved at /content/dataset_c.json with 2248 pairs.\n"
     ]
    }
   ],
   "source": [
    "# Concating synthesized data\n",
    "\n",
    "GDRIVE_PATH_1 = \"/content/drive/MyDrive/generated_pairs\"\n",
    "GDRIVE_PATH_2 = \"/content/drive/MyDrive/generated_pairs_2\"\n",
    "OUTPUT_PATH = \"/content/dataset_b.json\"\n",
    "MAX_UNIQUE_PAIRS = 1600\n",
    "\n",
    "# Function to clean the file content and extract the JSON data\n",
    "def clean_json_content(raw_content):\n",
    "    \"\"\"Removes non-JSON content from the beginning and end of the raw file content.\"\"\"\n",
    "    start_index = raw_content.find('[')\n",
    "    end_index = raw_content.rfind(']') + 1\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        return raw_content[start_index:end_index]\n",
    "    return None\n",
    "\n",
    "# Function to read all .txt files and check for duplicates\n",
    "def read_and_deduplicate_files(gdrive_paths, max_pairs=MAX_UNIQUE_PAIRS):\n",
    "    all_sentences = set()\n",
    "    combined_data = []\n",
    "\n",
    "    for path in gdrive_paths:\n",
    "        txt_files = [f for f in os.listdir(path) if f.endswith(\".txt\")]\n",
    "\n",
    "        for txt_file in txt_files:\n",
    "            file_path = os.path.join(path, txt_file)\n",
    "\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                raw_content = file.read()\n",
    "\n",
    "                cleaned_content = clean_json_content(raw_content)\n",
    "\n",
    "                if cleaned_content is None:\n",
    "                    print(f\"Error decoding JSON in file: {file_path}\")\n",
    "                    continue\n",
    "\n",
    "                if cleaned_content in all_sentences:\n",
    "                    print(f\"Duplicate response detected in file: {file_path}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # Add cleaned content to all_sentences and then parse it\n",
    "                all_sentences.add(cleaned_content)\n",
    "\n",
    "                try:\n",
    "                    json_data = json.loads(cleaned_content)\n",
    "                    combined_data.extend(json_data)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON in file: {file_path}. Error: {e}\")\n",
    "                    continue\n",
    "\n",
    "                if len(all_sentences) >= max_pairs:\n",
    "                    print(f\"Reached {max_pairs} unique pairs. Stopping.\")\n",
    "                    return combined_data\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "combined_data = read_and_deduplicate_files([GDRIVE_PATH_1, GDRIVE_PATH_2])\n",
    "\n",
    "with open(OUTPUT_PATH, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(combined_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Dataset B has been successfully saved at {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437,
     "referenced_widgets": [
      "92f2a125816a448c870cf72d7e0d7420",
      "e7f9946c10294a9da8d41b53cbd374af",
      "7869b823a715456da0cf2dc0d47db2fe",
      "04162ba52da24a56874334ab30ef94ae",
      "01b4052953674aa7b38455e17edef643",
      "e0277d4c04df40f1af7eb71657a5294f",
      "bbb7a374566c42fdb0d215af7e958d94",
      "08d7a3a5de364f5686ac9ea51dedb072",
      "f530142b857645c2a7c9022fa997d833",
      "9e744720e30a48fda8b153cb1cc87461",
      "71b845365d3e4e639923ff5eaaaf6b56",
      "08cf168f661a4afab5ce2124469fa816",
      "62045fa9710f455f82158dfe7f641181",
      "e2f2f6ab77c64dcb80716667548faae4",
      "c5baba50b2984d02b87beeb60b60bada",
      "d95c12a84cb8491a8079d42f4e9e1b25",
      "7fc78dcebb104cf39cb324753b38d73c",
      "0c94e156c6f745aabea29726a648157b",
      "520eecab06ea4f6fb73469fc4fe208af",
      "d31be9e093e64028bf2824a6a7926c81",
      "c84e6b70af9f49d09c33a8baf6f8e415",
      "18490e3cfbae441d90500a1ffe29359a"
     ]
    },
    "id": "2HfP8rUJ4b_k",
    "outputId": "91b50bfd-5cb8-4c0e-b6b6-126de3e46190"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f2a125816a448c870cf72d7e0d7420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08cf168f661a4afab5ce2124469fa816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-1cb7b0f3d5c9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Load the model with quantization configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Set up LoRA configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4243\u001b[0m                     \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4244\u001b[0m                     \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4245\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4246\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4247\u001b[0m                     \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[1;32m   4813\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4814\u001b[0m                         \u001b[0mfixed_state_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fix_state_dict_keys_on_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4815\u001b[0;31m                         new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n\u001b[0m\u001b[1;32m   4816\u001b[0m                             \u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4817\u001b[0m                             \u001b[0mfixed_state_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    873\u001b[0m             \u001b[0mset_module_tensor_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mset_module_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m             \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_quantized_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m             \u001b[0;31m# For quantized modules with FSDP/DeepSpeed Stage 3, we need to quantize the parameter on the GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;31m# and then cast it to CPU to avoid excessive memory usage on each GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_8bit.py\u001b[0m in \u001b[0;36mcreate_quantized_param\u001b[0;34m(self, model, param_value, param_name, target_device, state_dict, unexpected_keys)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInt8Params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             new_param = Int8Params(\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0;31m# We quantize the weight and store in 8bit row-major\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m             \u001b[0mCB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSCB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8_vectorwise_quant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fine-tune model on synthesize data\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Load the tokenizer and model for microsoft/phi-2\n",
    "model_name = \"microsoft/phi-2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the synthetic dataset (Dataset B)\n",
    "dataset_b_path = \"/content/dataset_b.json\"\n",
    "with open(dataset_b_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset_b = json.load(f)\n",
    "\n",
    "\n",
    "dataset_b = dataset_b[:1600]\n",
    "dataset_b = Dataset.from_list(dataset_b)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"german\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_dataset_b = dataset_b.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set up quantization configuration for 8-bit loading\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "# Load the model with quantization configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quant_config)\n",
    "\n",
    "# Set up LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model and freeze base parameters\n",
    "model = get_peft_model(model, peft_config)\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Data collator for causal language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Fine-tuning configuration\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./phi-2-finetuned-c\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-5,\n",
    "    fp16=True,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs/model_C_logs\",\n",
    "    logging_steps=100,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset_b,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Check that there are trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "if trainable_params == 0:\n",
    "    raise ValueError(\"No parameters are trainable. Check your LoRA configuration.\")\n",
    "\n",
    "print(\"Starting fine-tuning on Dataset B to create Model C...\")\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./my_finetuned_phi_2_synthesized\")\n",
    "print(\"Fine-tuning complete. Model saved as Model C.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aNdCz54tv9gV",
    "outputId": "7be03276-fa79-4ae1-ea1f-fcd9f316a636"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to /content/drive/MyDrive/my_finetuned_phi_2_synthesized\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "save_path = \"/content/drive/MyDrive/my_finetuned_phi_2_synthesized\"\n",
    "\n",
    "local_model_path = \"./my_finetuned_phi_2_synthesized\"\n",
    "\n",
    "tokenizer.save_pretrained(local_model_path)\n",
    "\n",
    "shutil.copytree(local_model_path, save_path)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "693c8849d95540e0a47420ee64fa2dba",
      "73f6bc10ecad41f4af8a63983bcda764",
      "4a4f9a0e5e5448edb1fb24f5086285b5",
      "f4b1c3a0d44c491b86953a17f8fc1e90",
      "9cf40be269544b21ad1886b94c6c0ece",
      "018887390a4d486b88e7e0fca0fac5e1",
      "91da5f6842584120b89dc4895e8a74d1",
      "a9358267403c47cd8f7ef5d6f4fd229f",
      "de06bec7196b4cd48e33bd65a7fe2291",
      "e4074c1e01994e24a43f9472b9c16bee",
      "4fb2487e1e9a413b8b15a323bffdd210",
      "f8d222f9768e44f9a059b98f173876ad",
      "8c4715bd5906473aa42054ee698c348a",
      "088cf68f3c7246189534aaae8fae85c1",
      "2b37b458a8bf4a01a96017448319bedc",
      "f765b97b4cc94f27931709fe92b70f14",
      "555bf8c9c413444686f557546333676c",
      "489aaad65c504022879687e3ec4fea42",
      "4195e525d86740e8a11118d869fb7e88",
      "58786df59dbf4113b51a5d988a85348e",
      "77e3f21f7bc6491f81917d97652832e5",
      "530f74d334164ed693d2202384c26972"
     ]
    },
    "id": "9c12teYafhXt",
    "outputId": "f97c0879-6890-420d-b5b1-25e5219e064e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693c8849d95540e0a47420ee64fa2dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d222f9768e44f9a059b98f173876ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluating model on synthesized data\n",
    "\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "model_path = \"/content/my_finetuned_phi_2_synthesized\"\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def generate_translations_batch(prompts, num_beams=4, max_length=150):\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        output_ids = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            repetition_penalty=1.2,\n",
    "            no_repeat_ngram_size=2,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        outputs = [tokenizer.decode(ids, skip_special_tokens=True) for ids in output_ids]\n",
    "    return outputs\n",
    "\n",
    "\n",
    "test_dataset = split_dataset[\"test\"]\n",
    "all_prompts = []\n",
    "all_references = []\n",
    "for example in test_dataset:\n",
    "    prompt = f\"translate German to French: {example['german']}\"\n",
    "    all_prompts.append(prompt)\n",
    "    all_references.append(example[\"target\"])\n",
    "\n",
    "\n",
    "predictions = []\n",
    "batch_size = 16\n",
    "for i in tqdm(range(0, len(all_prompts), batch_size), desc=\"Evaluating Model C\"):\n",
    "    batch_prompts = all_prompts[i: i + batch_size]\n",
    "    batch_outputs = generate_translations_batch(batch_prompts, num_beams=4, max_length=150)\n",
    "    for prompt, output in zip(batch_prompts, batch_outputs):\n",
    "        if prompt in output:\n",
    "            output = output.replace(prompt, \"\")\n",
    "        output = output.strip().split(\"\\n\")[0].strip()\n",
    "        predictions.append(output)\n",
    "\n",
    "# Compute BLEU Score Using SacreBLEU\n",
    "if predictions:\n",
    "    bleu = sacrebleu.corpus_bleu(predictions, [all_references])\n",
    "    print(\"SacreBLEU Score for Model C:\", bleu.score)\n",
    "else:\n",
    "    print(\"No valid translations generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BnlnzFuZwUVu",
    "outputId": "5e6559ba-e7d9-4459-907d-a70db097f9cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset A Train saved at /content/dataset_a_train.json with 800 pairs\n",
      "Dataset A Test saved at /content/dataset_a_test.json with 200 pairs\n",
      "Dataset A Train size: 800 pairs\n",
      "Dataset B size: 1561 pairs\n",
      "Combined dataset size (before deduplication): 2361 pairs\n",
      "Unique combined dataset size: 2248 pairs\n",
      "Dataset C saved at /content/dataset_c.json with 2248 pairs.\n"
     ]
    }
   ],
   "source": [
    "# Concating both datasets and shuffle\n",
    "\n",
    "dataset_a_train_path = \"/content/dataset_a_train.json\"\n",
    "dataset_a_test_path = \"/content/dataset_a_test.json\"\n",
    "dataset_b_path = \"/content/dataset_b.json\"\n",
    "dataset_c_path = \"/content/dataset_c.json\"\n",
    "\n",
    "# Save Dataset A\n",
    "if not os.path.exists(dataset_a_train_path):\n",
    "    with open(dataset_a_train_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(list(split_dataset[\"train\"]), f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Dataset A Train saved at {dataset_a_train_path} with {len(split_dataset['train'])} pairs\")\n",
    "else:\n",
    "    print(f\"Dataset A Train already exists at {dataset_a_train_path}\")\n",
    "\n",
    "if not os.path.exists(dataset_a_test_path):\n",
    "    with open(dataset_a_test_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(list(split_dataset[\"test\"]), f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Dataset A Test saved at {dataset_a_test_path} with {len(split_dataset['test'])} pairs\")\n",
    "else:\n",
    "    print(f\"Dataset A Test already exists at {dataset_a_test_path}\")\n",
    "\n",
    "def clean_json_content(raw_content):\n",
    "    \"\"\"Removes non-JSON content from the beginning and end of the raw file content.\"\"\"\n",
    "    start_index = raw_content.find('[')\n",
    "    end_index = raw_content.rfind(']') + 1\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        return raw_content[start_index:end_index]\n",
    "    return None\n",
    "\n",
    "def read_and_deduplicate_files(gdrive_paths, max_pairs=None):\n",
    "    all_sentences = set()\n",
    "    combined_data = []\n",
    "\n",
    "    for path in gdrive_paths:\n",
    "        txt_files = [f for f in os.listdir(path) if f.endswith(\".txt\")]\n",
    "        for txt_file in txt_files:\n",
    "            file_path = os.path.join(path, txt_file)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                raw_content = file.read()\n",
    "                cleaned_content = clean_json_content(raw_content)\n",
    "                if cleaned_content is None:\n",
    "                    print(f\"Error decoding JSON in file: {file_path}\")\n",
    "                    continue\n",
    "                if cleaned_content in all_sentences:\n",
    "                    print(f\"Duplicate response detected in file: {file_path}. Skipping.\")\n",
    "                    continue\n",
    "                all_sentences.add(cleaned_content)\n",
    "                try:\n",
    "                    json_data = json.loads(cleaned_content)\n",
    "                    combined_data.extend(json_data)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON in file: {file_path}. Error: {e}\")\n",
    "                    continue\n",
    "                if max_pairs and len(all_sentences) >= max_pairs:\n",
    "                    print(f\"Reached {max_pairs} unique pairs. Stopping.\")\n",
    "                    return combined_data\n",
    "    return combined_data\n",
    "\n",
    "# Combine Dataset A Train and Dataset B to form Dataset C\n",
    "with open(dataset_a_train_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset_a_train = json.load(f)\n",
    "with open(dataset_b_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset_b = json.load(f)\n",
    "\n",
    "print(f\"Dataset A Train size: {len(dataset_a_train)} pairs\")\n",
    "print(f\"Dataset B size: {len(dataset_b)} pairs\")\n",
    "\n",
    "combined_dataset = dataset_a_train + dataset_b\n",
    "print(f\"Combined dataset size (before deduplication): {len(combined_dataset)} pairs\")\n",
    "\n",
    "# Deduplicate based on German and French content\n",
    "unique_pairs = {}\n",
    "for entry in combined_dataset:\n",
    "    german = entry.get(\"German\") or entry.get(\"german\")\n",
    "    french = entry.get(\"French\") or entry.get(\"french\")\n",
    "    if german is None or french is None:\n",
    "        continue\n",
    "    key = (german.strip(), french.strip())\n",
    "    unique_pairs[key] = {\"German\": german.strip(), \"French\": french.strip()}\n",
    "\n",
    "combined_unique_dataset = list(unique_pairs.values())\n",
    "print(f\"Unique combined dataset size: {len(combined_unique_dataset)} pairs\")\n",
    "\n",
    "# Shuffle the combined unique dataset with a fixed seed for reproducibility\n",
    "random.seed(42)\n",
    "random.shuffle(combined_unique_dataset)\n",
    "\n",
    "with open(dataset_c_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(combined_unique_dataset, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Dataset C saved at {dataset_c_path} with {len(combined_unique_dataset)} pairs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428,
     "referenced_widgets": [
      "1a902098231c402987d528f77aa02079",
      "2e090ab463b541acb8554337c5ab4e21",
      "e7bc525c94f84278b717356056260f14",
      "8d7d8fcc54804e858b74d9b4018986ab",
      "b19b2b0690fb4c1b95d86f42c8ba39b9",
      "444f52a0f38041f58c3c86451df89560",
      "2229e5c4cb0141c6bb5340c557e0ca55",
      "18bac4de51324dd9835dee11518be135",
      "3d0a056cd2fc4a0b85a77f27716b5971",
      "4f223e01866c48d1969ae7eb8c1a8f2f",
      "01736ec6f41742368c794462dc4893fa",
      "bf2b7a028dcd459aac7b508831db08f3",
      "6eb541ad01604733aeb1689ffd9592ba",
      "37a317bf17794994a2c014c6b198297f",
      "2ecb30a802564e57a665ba1b69cb678b",
      "6e437f71cdb34f30a7ca223e6c2b5cce",
      "132fcb2433864461bd6f0afb4e401ca1",
      "02a1f7f8889e49e2a8e2fad38d179ad5",
      "cddbfcb22268476497900ec71344e244",
      "bf7c7ff36bd64343afdd580eba05207b",
      "dc32ae70514441ed842ef7b80d68b2d2",
      "9ef026a0be9f4c48ba2b654674e49113"
     ]
    },
    "id": "trjKjCmuwrpo",
    "outputId": "48a06598-085d-4c4c-c354-98206c8b56b2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a902098231c402987d528f77aa02079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2391 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2b7a028dcd459aac7b508831db08f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 36700160\n",
      "Starting fine-tuning on Dataset C to create Model D...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='897' max='897' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [897/897 45:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.392900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.979100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.892000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning complete. Model saved as Model D.\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune model on combined dataset\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "dataset_c_path = \"/content/dataset_c.json\"\n",
    "with open(dataset_c_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "    dataset_c = json.load(json_file)\n",
    "\n",
    "# Convert Dataset C into a Hugging Face Dataset\n",
    "dataset_c = Dataset.from_list(dataset_c)\n",
    "\n",
    "model_name = \"microsoft/phi-2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    texts = [str(text) for text in examples[\"German\"]]\n",
    "    return tokenizer(texts, truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_dataset_c = dataset_c.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set up quantization configuration for 8-bit precision\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "# Load the model with quantization configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quant_config)\n",
    "\n",
    "# Set up the LoRA configuration for fine-tuning\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "# Apply LoRA configuration to the model\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Freeze base model parameters except for LoRA layers\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./phi-2-finetuned-on-combined\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-5,\n",
    "    fp16=True,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs/model_D_logs\",\n",
    "    logging_steps=200,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset_c,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "if trainable_params == 0:\n",
    "    raise ValueError(\"No parameters are trainable. Check your LoRA configuration.\")\n",
    "\n",
    "print(\"Starting fine-tuning on Dataset C to create Model D...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model (Model D)\n",
    "trainer.save_model(\"./my_finetuned_phi_2_on_combined\")\n",
    "print(\"Fine-tuning complete. Model saved as Model D.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1VfKPEadHgjI",
    "outputId": "04b4367e-36df-4770-e4d2-27d57a66f86d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to /content/drive/MyDrive/my_finetuned_phi_2_on_combined\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "save_path = \"/content/drive/MyDrive/my_finetuned_phi_2_on_combined\"\n",
    "\n",
    "local_model_path = \"./my_finetuned_phi_2_on_combined\"\n",
    "\n",
    "tokenizer.save_pretrained(local_model_path)\n",
    "\n",
    "shutil.copytree(local_model_path, save_path)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351,
     "referenced_widgets": [
      "6080376d602c4e05a61dc16444949d35",
      "fefefaaf3d334492851beb039c4a2688",
      "c11fbd3386504a94ab5de23d883f9e53",
      "7cf494da4d4f42a7b721a3d18e360482",
      "31c03ef9ad7d4c0d9a41b9ad2536e8f4",
      "b27dbdd28a014a629c98c4aeaeeb8598",
      "20e010b0b6db419f973932d15ef0714c",
      "37efd435fd324effabd7e4a9d5372e66",
      "dd23d5592d574e29a7db4e6ce47c26ea",
      "4f3cfcfdd0004f0b97246cfc49fed69e",
      "07f1f0c21efa4e1b818dee231e85305c",
      "2de5aef4f10c4359a4692498ce7c574f",
      "0691c16bb41c437b9ba743dff0f002b1",
      "ed6bfc377d25476ba5ec95262ac28cd0",
      "b449faef6fd54a0da61b6339affe9fea",
      "06739c27957e4475985cc7913d82692d",
      "d766a2e7e2874418bbf7c152e1a35cf6",
      "cde5fa1904e3471b8b3009ab4245ddd2",
      "18078a0d046a419f82cc644aeb5c8da9",
      "584416d5eb5c48f18b5bb274e9ce6308",
      "90cedcffb22d49208e39fe7b7d8f9a56",
      "fe97143d36b54f33b2d3de1628642cdf"
     ]
    },
    "id": "vx7dsGCyeiJk",
    "outputId": "2f923cba-c047-48ce-ad9c-e1ae23b2ae1f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6080376d602c4e05a61dc16444949d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de5aef4f10c4359a4692498ce7c574f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Model D:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=50) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=50) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=50) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=50) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=50) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=50) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=50) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=50) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=50) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=50) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=50) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=50) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=50) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SacreBLEU Score for Model D: 7.624196513770272e-172\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on combined data\n",
    "\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "model_path = \"/content/drive/MyDrive/my_finetuned_phi_2_on_combined\"\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "def generate_translations_batch(prompts, num_beams=4, max_length=128, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    Generates translations for a batch of input prompts.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        output_ids = model.generate(\n",
    "            inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            repetition_penalty=1.2,\n",
    "            no_repeat_ngram_size=2,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            max_new_tokens=max_new_tokens  # Add this parameter for controlling the new token length\n",
    "        )\n",
    "        outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    return outputs\n",
    "\n",
    "test_dataset = split_dataset[\"test\"]\n",
    "\n",
    "all_prompts = [f\"Translate the following German sentence into French:\\n\\nGerman: {ex['german']}\\n\\nFrench:\"\n",
    "               for ex in test_dataset]\n",
    "all_references = [ex[\"target\"] for ex in test_dataset]\n",
    "\n",
    "predictions = []\n",
    "batch_size = 16\n",
    "for i in tqdm(range(0, len(all_prompts), batch_size), desc=\"Evaluating Model D\"):\n",
    "    batch_prompts = all_prompts[i: i + batch_size]\n",
    "    batch_outputs = generate_translations_batch(batch_prompts, num_beams=4, max_length=128, max_new_tokens=50)\n",
    "\n",
    "    for prompt, output in zip(batch_prompts, batch_outputs):\n",
    "        output = output.replace(prompt, \"\").strip().split(\"\\n\")[0].strip()\n",
    "        predictions.append(output)\n",
    "\n",
    "if predictions:\n",
    "    bleu_result = bleu_metric.compute(predictions=predictions, references=[[ref] for ref in all_references])\n",
    "    print(\"SacreBLEU Score for Model D:\", bleu_result[\"score\"])\n",
    "else:\n",
    "    print(\"No valid translations generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5A1jD5I9Zmc0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
