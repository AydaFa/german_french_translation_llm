# german_french_translation_llm

## Table of Contents
* [Introduction](#introduction)
* [Requirements](#requirements)
* [Setup](#setup)
* [Training and Usage](#training-and-usage)
* [Acknowledgments](#acknowledgments)

## Introduction
This project implements a language translation system by fine-tuning a large language model (LLM) for German-to-French translation. It demonstrates how a general-purpose pre-trained model can be adapted to a specific translation task using a custom benchmark dataset and synthetic data. The project includes evaluation of the model at various stages to monitor performance improvement through each fine-tuning cycle.

## Requirements
To run this project, you will need the following:

- Python version: 3.10+
- Transformers (HuggingFace)
- Datasets (HuggingFace)
- Accelerate
- PEFT (Parameter-Efficient Fine-Tuning)
- BitsAndBytes
- Evaluate
- Matplotlib
- Google Colab (with T4 GPU)

Install all packages using:
```bash
pip install -q transformers datasets accelerate peft bitsandbytes evaluate matplotlib
pip install -q transformers datasets accelerate peft bitsandbytes evaluate matplotlib
```
## Setup

Follow these steps to set up and run the project:

1. Clone this repository to your local machine:
```bash
git clone https://github.com/your-username/german_french_translation_llm.git
cd german_french_translation_llm
```
2. Ensure you have Python and all required dependencies installed.
3. Open the Jupyter notebook GenAI.ipynb in Google Colab.
4. Follow the notebook cells to execute each phase of the fine-tuning and evaluation.

## Training and Usage

The project includes the following key components:

- Data Preparation  
  A benchmark dataset of 1,000 German-French translation pairs is used. The dataset is split into training and testing sets. Synthetic data is also generated by querying a large LLM (Qwen2.5-Coder-32B-Instruct) with custom prompts.

- Model Architecture  
  A pre-trained LLM (~1B parameters) is selected. PEFT with LoRA is used to fine-tune the model efficiently within Google Colabâ€™s memory constraints.

- Training Phases  
  - Model A: Base model evaluation on the test set.  
  - Model B: Fine-tuned on the benchmark training data.  
  - Model C: Fine-tuned on synthetic data generated from Model A.  
  - Model D: Fine-tuned on the combination of benchmark and synthetic data.
 
- Evaluation
  - BLEU score is used to evaluate translation quality at every stage. Performance metrics are visualized using Matplotlib.
 
- Example Usage
from transformers import pipeline
```bash
translator = pipeline("translation", model="path/to/your/final-model")
result = translator("Guten Morgen! Wie geht es dir?")
print(result)
```
## Acknowledgments
This project is built using HuggingFace Transformers, Datasets, PEFT, and Evaluate libraries. It was developed and tested using Google Colab with free T4 GPU access. Special thanks to the Qwen2.5-Coder-32B-Instruct model for generating high-quality synthetic translation data.
